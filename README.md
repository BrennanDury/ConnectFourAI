# ConnectFourAI
This project creates Connect Four AI using a variety of algorithms. AI strategies vary in sophistication from a simple minimax with a heuristic based on domain knowledge to an implementation of AlphaZero.

The AlphaZero implementation is located in the AlphaZero folder, while all other strategies are located in the OtherStrategies folder. The AlphaZero implementation is also fully contained in AlphaZeroC4.ipynb

The AlphaZero strategy uses a Monte Carlo search tree to explore promising moves according to a Convolutional Neural Network. 200 searches are performed to calculate the best move. At the end of the searches, the chosen move is sampled from the number of visits to each child of the root node taken as an improved probability vector. When traversing the tree, from a given board state, the most promising move to explore is the move that maximizes q(a) + c * p(a) * sqrt(n(b)) / (n(a) + 1), where q is the current estimation of the quality of a board, c is a hyperparameter controlling the amount of exploration vs exploitation, set to 1, p is the prior evaluation of a the probability of making the move leading to the board according the neural network, n is the number of times a board has been visited, a is the board state after an action, and b is the current board state. This function balances exploration of new information vs exploitation of current information.

A single search builds a game tree until a new board state is found or the simulated game is won. If a new board state is found, the search tree inferences the neural net for an evaluation of the resulting board of each possible next move. The evaluation of the board is backpropegated towards the root board, updating q for each board that led to the end of this search. If the simulated game is won, the result is backpropegated instead. Each board that occurs in the real game is added to the training data, along with the result and the number of simulated visits to each next move. The network is retrained after every 100 games.

The CNN takes a board as input and returns the quality of that board and an initial probability vector of the next move. The network is built of repeated residual layers. Each residual layer has the same input and output shape: (75, 7, 6, 1). The 75 represents the number of filters for each 4x4 convolution, while 7x6 are the dimensions of the board and 1 represents that there is just one color channel. Then there is a value head to transform the output of the last residual layer into a initial q value for the board and a policy head to transform the output of the last residual layer into the probability vector.

The most basic strategy uses minimax to build a search tree. Positions at the max depth are evaluated with a heuristic that counts the number of fours, open threes, open twos, and open ones. The minimax algorithm includes alpha beta pruning, a transposition table, iterative deepening, and move ordering with a cheap heuristic to improve search time and by extension, increase max depth.

A simple but effective strategy is a basic Monte Carlo search tree. Instead of applying a heuristic, run fully random simulations from a given board until the game ends and choose the move with the highest win rate. This strategy produces a reasonable human level player. It is likely to miss sequences which require precise play, but is capable of making open connections, blocking its opponentâ€™s open connections, prioritizing the center, and ignoring unreachable connections.

The MLP and Perceptron files contain my own implementations of neural networks trained with an evolutionary algorithm in java without using libraries. I created a working neural network and later transitioned to TensorFlow for the AlphaZero implementation to take advantage of gpu optimization.

